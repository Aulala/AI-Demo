{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![設計你的神經網路](images/create_your_NN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們終於要開始做生命中第一個神經網路..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 讀入套件\n",
    "\n",
    "這裡我們讀入一些套件, 今天暫時不要理會細節。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再來是我們標準數據分析動作!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: KERAS_BACKEND=tensorflow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%env KERAS_BACKEND=tensorflow\n",
    "\n",
    "# 標準數據分析、畫圖套件\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 神經網路方面\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# 互動設計用\n",
    "from ipywidgets import interact_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 讀入 MNIST 數據庫\n",
    "\n",
    "MNIST 是有一堆 0-9 的手寫數字圖庫。有 6 萬筆訓練資料, 1 萬筆測試資料。它是 \"Modified\" 版的 NIST 數據庫, 原來的版本有更多資料。這個 Modified 的版本是由 LeCun, Cortes, 及 Burges 等人做的。可以參考這個數據庫的[原始網頁](http://yann.lecun.com/exdb/mnist/)。\n",
    "\n",
    "MNIST 可以說是 Deep Learning 最有名的範例, 它被 Deep Learning 大師 Hinton 稱為「機器學習的果蠅」。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 由 Keras 讀入 MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras 很貼心的幫我們準備好 MNIST 數據庫, 我們可以這樣讀進來 (第一次要花點時間)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們來看看訓練資料是不是 6 萬筆、測試資料是不是有 1 筆。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練資料總筆數為 60000 筆資料\n",
      "測試資料總筆數為 10000 筆資料\n"
     ]
    }
   ],
   "source": [
    "print(f'訓練資料總筆數為 {len(x_train)} 筆資料')\n",
    "print(f'測試資料總筆數為 {len(x_test)} 筆資料')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 數據庫的內容\n",
    "\n",
    "每筆輸入 (x) 就是一個手寫的 0-9 中一個數字的圖檔, 大小為 28x28。而輸出 (y) 當然就是「正確答案」。我們來看看編訓練資料的 x 輸入、輸出的部份分別長什麼樣子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_xy(n=0):\n",
    "    ax = plt.gca()\n",
    "    X = x_train[n]\n",
    "    plt.xticks([], [])\n",
    "    plt.yticks([], [])\n",
    "    plt.imshow(X, cmap = 'Greys')\n",
    "    print(f'本資料 y 給定的答案為: {y_train[n]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "836ab9888910441586a8c39aaf3013df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='n', max=59999), Button(description='Run Interact', style…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interact_manual(show_xy, n=(0,59999));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "X = x_train[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          2,  18,  46, 136, 136, 244, 255, 241, 103,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  15,  94,\n",
       "        163, 253, 253, 253, 253, 238, 218, 204,  35,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 131, 253,\n",
       "        253, 253, 253, 237, 200,  57,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 155, 246, 253,\n",
       "        247, 108,  65,  45,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 207, 253, 253,\n",
       "        230,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 157, 253, 253,\n",
       "        125,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  89, 253, 250,\n",
       "         57,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  89, 253, 247,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  89, 253, 247,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  89, 253, 247,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  21, 231, 249,\n",
       "         34,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 225, 253,\n",
       "        231, 213, 213, 123,  16,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 172, 253,\n",
       "        253, 253, 253, 253, 190,  63,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   2, 116,\n",
       "         72, 124, 209, 253, 253, 141,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  25, 219, 253, 206,   3,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0, 104, 246, 253,   5,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 213, 253,   5,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,  26, 226, 253,   5,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0, 132, 253, 209,   3,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,  78, 253,  86,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 輸入格式整理\n",
    "\n",
    "我們現在要用標準神經網路學學手寫辨識。原來的每筆數據是個 28x28 的矩陣 (array), 但標準神經網路只吃「平平的」, 也就是每次要 28x28=784 長的向量。因此我們要用 `reshape` 調校一下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(60000, 784)/255\n",
    "x_test = x_test.reshape(10000, 784)/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 輸出格式整理\n",
    "\n",
    "我們可能會想, 我們想學的函數是這樣的型式:\n",
    "\n",
    "$$\\hat{f} \\colon \\mathbb{R}^{784} \\to \\mathbb{R}$$\n",
    "\n",
    "其實這樣不太好! 為什麼呢? 比如說我們的輸入 x 是一張 0 的圖, 因為我們訓練的神經網路總會有點誤差, 所以可能會得到:\n",
    "\n",
    "$$\\hat{f}(x) = 0.5$$\n",
    "\n",
    "那這意思是有可能是 0, 也有可能是 1 嗎!!?? 可是 0 和 1 根本不像啊。換句話說分類的問題這樣做其實不合理!\n",
    "\n",
    "於是我們會做 \"1-hot enconding\", 也就是\n",
    "\n",
    "* 1 -> [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "* 5 -> [0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "\n",
    "等等。因為分類問題基本上都要做這件事, Keras 其實已幫我們準備好套件!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np_utils.to_categorical(y_train, 10)\n",
    "y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們來看看剛剛某號數據的答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 87\n",
    "y_train[n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和我們想的一樣! 至此我們可以打造我們的神經網路了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 打造第一個神經網路\n",
    "\n",
    "我們決定了我們的函數是\n",
    "\n",
    "$$\\hat{f} \\colon \\mathbb{R}^{784} \\to \\mathbb{R}^{10}$$\n",
    "\n",
    "這個樣子。而我們又說第一次要用標準神網路試試, 所以我們只需要再決定要幾個隱藏層、每層要幾個神經元, 用哪個激發函數就可以了。\n",
    "\n",
    "### 3.1 決定神經網路架構、讀入相關套件\n",
    "\n",
    "假如我們要這麼做:\n",
    "\n",
    "* 使用 <span style=\"color:red;\"> 2 </span> 個 hidden layers\n",
    "* Hidden layer 1 用 <span style=\"color:red;\"> 50 </span> 個神經元\n",
    "* Hidden layer 2 用 <span style=\"color:red;\"> 100 </span> 個神經元\n",
    "\n",
    "* Activation Function 唯一指名 <span style=\"color:red;\">relu</span>\n",
    "\n",
    "於是從 Keras 把相關套件讀進來。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 建構我們的神經網路\n",
    "\n",
    "和以前做迴歸或機器學習一樣, 我們就打開個「函數學習機」。標準一層一層傳遞的神經網路叫 `Sequential`, 於是我們打開一個空的神經網路。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們每次用 `add` 去加一層, 從第一個隱藏層開始。而第一個隱藏層因為 Keras 當然猜不到輸入有幾個 features, 所以我們要告訴它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(50, input_dim=784, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二層 hidden layer 因為前面輸出是 4, 現在輸入是 2, 就不用再說了! 這裡的 2 只告訴 Keras, 我們第二層是用 2 個神經元!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(100, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "輸出有 10 個數字, 所以輸出層的神經元是 10 個! 而如果我們的網路輸出是 \n",
    "\n",
    "$$(y_1, y_2, \\ldots, y_{10})$$\n",
    "\n",
    "我們還希望\n",
    "\n",
    "$$\\sum_{i=1}^{10} y_i = 1$$\n",
    "\n",
    "這可能嗎, 結果是很容易, 就用 `softmax` 當激發函數就可以!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此我們的第一個神經網路就建好了!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 組裝\n",
    "\n",
    "和之前比較不一樣的是我們還要做 `compile` 才正式把我們的神經網路建好。你可以發現我們還需要做幾件事:\n",
    "\n",
    "* 決定使用的 loss function, 一般是 `mse`\n",
    "* 決定 optimizer, 我們用標準的 SGD\n",
    "* 設 learning rate\n",
    "\n",
    "為了一邊訓練一邊看到結果, 我們加設\n",
    "\n",
    "    metrics=['accuracy']\n",
    "    \n",
    "本行基本上和我們的神經網路功能沒有什麼關係。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer=SGD(lr=0.001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 檢視我們的神經網路"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們可以檢視我們神經網路的架構, 可以確認一下是不是和我們想像的一樣。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 看 model 的 summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               5100      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 45,360\n",
      "Trainable params: 45,360\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "很快算算參數數目和我們想像是否是一樣的!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7850"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "784*10 + 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 訓練你的第一個神經網路\n",
    "\n",
    "恭喜! 我們完成了第一個神經網路。現在要訓練的時候, 你會發現不是像以前沒頭沒腦把訓練資料送進去就好。這裡我們還有兩件事要決定:\n",
    "\n",
    "* 一次要訓練幾筆資料 (`batch_size`), 我們就 100 筆調一次參數好了\n",
    "* 這 6 萬筆資料一共要訓練幾次 (`epochs`), 我們訓練個 20 次試試\n",
    "\n",
    "於是最精彩的就來了。你要有等待的心理準備..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0890 - acc: 0.2183\n",
      "Epoch 2/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0889 - acc: 0.2240\n",
      "Epoch 3/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0889 - acc: 0.2294\n",
      "Epoch 4/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0888 - acc: 0.2345\n",
      "Epoch 5/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0888 - acc: 0.2395\n",
      "Epoch 6/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0887 - acc: 0.2447\n",
      "Epoch 7/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0887 - acc: 0.2492\n",
      "Epoch 8/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0886 - acc: 0.2534\n",
      "Epoch 9/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0886 - acc: 0.2576\n",
      "Epoch 10/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0885 - acc: 0.2612\n",
      "Epoch 11/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0885 - acc: 0.2654\n",
      "Epoch 12/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0884 - acc: 0.2691\n",
      "Epoch 13/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0883 - acc: 0.2730\n",
      "Epoch 14/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0883 - acc: 0.2759\n",
      "Epoch 15/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0882 - acc: 0.2786\n",
      "Epoch 16/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0882 - acc: 0.2822\n",
      "Epoch 17/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0881 - acc: 0.2854\n",
      "Epoch 18/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0880 - acc: 0.2885\n",
      "Epoch 19/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0880 - acc: 0.2912\n",
      "Epoch 20/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0879 - acc: 0.2939\n",
      "Epoch 21/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0879 - acc: 0.2965\n",
      "Epoch 22/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0878 - acc: 0.2988\n",
      "Epoch 23/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0877 - acc: 0.3013\n",
      "Epoch 24/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0877 - acc: 0.3036\n",
      "Epoch 25/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0876 - acc: 0.3063\n",
      "Epoch 26/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0875 - acc: 0.3081\n",
      "Epoch 27/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0874 - acc: 0.3100\n",
      "Epoch 28/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0874 - acc: 0.3124\n",
      "Epoch 29/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0873 - acc: 0.3143\n",
      "Epoch 30/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0872 - acc: 0.3157\n",
      "Epoch 31/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0871 - acc: 0.3173\n",
      "Epoch 32/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0871 - acc: 0.3191\n",
      "Epoch 33/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0870 - acc: 0.3211\n",
      "Epoch 34/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0869 - acc: 0.3227\n",
      "Epoch 35/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0868 - acc: 0.3239\n",
      "Epoch 36/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0867 - acc: 0.3254\n",
      "Epoch 37/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0866 - acc: 0.3271\n",
      "Epoch 38/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0865 - acc: 0.3280\n",
      "Epoch 39/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0865 - acc: 0.3293\n",
      "Epoch 40/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0864 - acc: 0.3307\n",
      "Epoch 41/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0863 - acc: 0.3323\n",
      "Epoch 42/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0862 - acc: 0.3335\n",
      "Epoch 43/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0861 - acc: 0.3348\n",
      "Epoch 44/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0860 - acc: 0.3363\n",
      "Epoch 45/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0858 - acc: 0.3372\n",
      "Epoch 46/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0857 - acc: 0.3384\n",
      "Epoch 47/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0856 - acc: 0.3395\n",
      "Epoch 48/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0855 - acc: 0.3410\n",
      "Epoch 49/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0854 - acc: 0.3425\n",
      "Epoch 50/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0853 - acc: 0.3439\n",
      "Epoch 51/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0851 - acc: 0.3452\n",
      "Epoch 52/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0850 - acc: 0.3465\n",
      "Epoch 53/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0849 - acc: 0.3474\n",
      "Epoch 54/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0847 - acc: 0.3483\n",
      "Epoch 55/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0846 - acc: 0.3495\n",
      "Epoch 56/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0845 - acc: 0.3510\n",
      "Epoch 57/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0843 - acc: 0.3525\n",
      "Epoch 58/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0842 - acc: 0.3541\n",
      "Epoch 59/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0840 - acc: 0.3559\n",
      "Epoch 60/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0839 - acc: 0.3576\n",
      "Epoch 61/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0837 - acc: 0.3589\n",
      "Epoch 62/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0835 - acc: 0.3604\n",
      "Epoch 63/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0834 - acc: 0.3619\n",
      "Epoch 64/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0832 - acc: 0.3636\n",
      "Epoch 65/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0830 - acc: 0.3654\n",
      "Epoch 66/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0829 - acc: 0.3673\n",
      "Epoch 67/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0827 - acc: 0.3693\n",
      "Epoch 68/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0825 - acc: 0.3716\n",
      "Epoch 69/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0823 - acc: 0.3738\n",
      "Epoch 70/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0821 - acc: 0.3763\n",
      "Epoch 71/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0819 - acc: 0.3784\n",
      "Epoch 72/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0817 - acc: 0.3804\n",
      "Epoch 73/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0815 - acc: 0.3833\n",
      "Epoch 74/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0813 - acc: 0.3856\n",
      "Epoch 75/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0811 - acc: 0.3883\n",
      "Epoch 76/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0809 - acc: 0.3907\n",
      "Epoch 77/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0807 - acc: 0.3933\n",
      "Epoch 78/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0805 - acc: 0.3959\n",
      "Epoch 79/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0803 - acc: 0.3987\n",
      "Epoch 80/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0801 - acc: 0.4014\n",
      "Epoch 81/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0799 - acc: 0.4041\n",
      "Epoch 82/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0796 - acc: 0.4070\n",
      "Epoch 83/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0794 - acc: 0.4099\n",
      "Epoch 84/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0792 - acc: 0.4127\n",
      "Epoch 85/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0789 - acc: 0.4154\n",
      "Epoch 86/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0787 - acc: 0.4184\n",
      "Epoch 87/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0785 - acc: 0.4210\n",
      "Epoch 88/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0782 - acc: 0.4237\n",
      "Epoch 89/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0780 - acc: 0.4264\n",
      "Epoch 90/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0777 - acc: 0.4294\n",
      "Epoch 91/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0775 - acc: 0.4318\n",
      "Epoch 92/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0772 - acc: 0.4351\n",
      "Epoch 93/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0770 - acc: 0.4381\n",
      "Epoch 94/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0767 - acc: 0.4411\n",
      "Epoch 95/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0765 - acc: 0.4436\n",
      "Epoch 96/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0762 - acc: 0.4465\n",
      "Epoch 97/100\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0759 - acc: 0.4491\n",
      "Epoch 98/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0757 - acc: 0.4513\n",
      "Epoch 99/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0754 - acc: 0.4540\n",
      "Epoch 100/100\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 0.0751 - acc: 0.4565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f04f7f75400>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=100, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 試用我們的結果\n",
    "\n",
    "我們來用比較炫的方式來看看可愛的神經網路學習成果。對指令有問題可以參考我們之前的 MOOC 影片教學。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們 \"predict\" 放的是我們神經網路的學習結果。這裡用 `predict_classes` 會讓我們 Keras 選 10 個輸出機率最大的那類。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 0, 1, ..., 9, 6, 6])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不要忘了我們的 `x_test` 每筆資料已經換成 784 維的向量, 我們要整型回 28x28 的矩陣才能當成圖形顯示出來!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(測試編號):\n",
    "    plt.imshow(x_test[測試編號].reshape(28,28), cmap='Greys')\n",
    "    print('神經網路判斷為:', predict[測試編號])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "神經網路判斷為: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test(87)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b945b792b5e34cdab54f25fa9573fbaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=4999, description='測試編號', max=9999), Button(description='Run Interact', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.test(測試編號)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact_manual(test, 測試編號=(0, 9999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到底測試資料總的狀況如何呢? 我們可以給我們神經網路「考一下試」。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 24us/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.08840728223323822\n",
      "正確率 0.217\n"
     ]
    }
   ],
   "source": [
    "print('loss:', score[0])\n",
    "print('正確率', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 訓練好的神經網路存起來!\n",
    "\n",
    "如果對訓練成果滿意, 我們當然不想每次都再訓練一次! 我們可以把神經網路的架構和訓練好的參數都存起來, 以供日後使用!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之前還沒裝 pyh5 要在終端機 (Anaconda Prompt) 下安裝:\n",
    "    \n",
    "    conda install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "open('stupid_model.json', 'w').write(model_json)\n",
    "model.save_weights('stupid_model_weights.h5')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
